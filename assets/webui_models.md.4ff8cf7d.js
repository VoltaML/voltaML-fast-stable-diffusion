import{_ as e,c as t,o as a,a as o}from"./app.25052814.js";const u=JSON.parse('{"title":"Models","description":"","frontmatter":{},"headers":[{"level":2,"title":"AITemplate compilation","slug":"aitemplate-compilation","link":"#aitemplate-compilation","children":[]},{"level":2,"title":"Float16 / Float32","slug":"float16-float32","link":"#float16-float32","children":[]},{"level":2,"title":"Model conversion","slug":"model-conversion","link":"#model-conversion","children":[]}],"relativePath":"webui/models.md","lastUpdated":1679945696000}'),d={name:"webui/models.md"},l=o('<h1 id="models" tabindex="-1">Models <a class="header-anchor" href="#models" aria-hidden="true">#</a></h1><p>VoltaML support all of these methods for loading models:</p><table><thead><tr><th></th><th>Diffusers</th><th>Checkpoint (.ckpt)</th><th>Safetensors (.safetensors)</th><th>AITemplate</th></tr></thead><tbody><tr><td>AITemplate compile</td><td>Yes</td><td>No</td><td>No</td><td>Unavaliable</td></tr><tr><td>Float 16</td><td>Yes</td><td>Only if already pruned</td><td>Only if already pruned</td><td>Yes</td></tr><tr><td>Float 32</td><td>Yes</td><td>Yes</td><td>Yes</td><td>No</td></tr></tbody></table><p>Both <code>Checkpoint</code> and <code>Safetensors</code> are loaded with float type that they were saved with. Fix needs to be done in the <code>diffusers</code> package - follow <a href="https://github.com/huggingface/diffusers/issues/2755" target="_blank" rel="noreferrer">this</a> thread for more info.</p><h2 id="aitemplate-compilation" tabindex="-1">AITemplate compilation <a class="header-anchor" href="#aitemplate-compilation" aria-hidden="true">#</a></h2><p>The AITemplate compilation is a process that traces the model and creates more optimized version of it. This process can be started on the <code>Accelerate</code> page.</p><h2 id="float16-float32" tabindex="-1">Float16 / Float32 <a class="header-anchor" href="#float16-float32" aria-hidden="true">#</a></h2><p>This refers to the precision of the model. Float16 is a half precision model, which is faster to load and run, but less accurate. Float32 is a full precision model, which is slower to load and run, but more accurate.</p><p>It can be also seen on the filesizes of the models. Float16 models are 2x smaller than Float32 models (2GB compared to 4GB).</p><h2 id="model-conversion" tabindex="-1">Model conversion <a class="header-anchor" href="#model-conversion" aria-hidden="true">#</a></h2><p>Work in progress.</p>',11),s=[l];function r(i,n,c,h,p,m){return a(),t("div",null,s)}const _=e(d,[["render",r]]);export{u as __pageData,_ as default};
